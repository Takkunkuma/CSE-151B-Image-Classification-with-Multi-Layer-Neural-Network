{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import constants\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_data(inp):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Normalizes inputs (on per channel basis of every image) here to have 0 mean and unit variance.\n",
    "    This will require reshaping to seprate the channels and then undoing it while returning\n",
    "\n",
    "    args:\n",
    "        inp : N X d 2D array where N is the number of examples and d is the number of dimensions\n",
    "\n",
    "    returns:\n",
    "        normalized inp: N X d 2D array\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # N X (32 * 32 * 3) to N X 32 * 32 X 3\n",
    "    d = int(inp.shape[1] / 3) # only works for square images\n",
    "    N = inp.shape[0]\n",
    "    per_channel = inp.reshape((N, d, 3))\n",
    "    \n",
    "\n",
    "    # normalize per channel per image\n",
    "    mu_per_channel_per_image = np.mean(per_channel, axis=1)\n",
    "    std_per_channel_per_image = np.std(per_channel, axis=1)\n",
    "    \n",
    "  \n",
    "    mu_2d = np.column_stack([np.tile(mu_per_channel_per_image[:, i].reshape((N, 1)), d) for i in range(3)])\n",
    "    std_2d = np.column_stack([np.tile(std_per_channel_per_image[:, i].reshape((N, 1)), d) for i in range(3)])\n",
    "\n",
    "\n",
    "    normalized = (inp - mu_2d) / std_2d\n",
    "    \n",
    "\n",
    "    return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train (40000, 3072)\n"
     ]
    }
   ],
   "source": [
    "t_x, t_y, v_x, v_y, ts_x, ts_y = load_data('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def one_hot_encoding(labels, num_classes=10):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Encodes labels using one hot encoding.\n",
    "\n",
    "    args:\n",
    "        labels : N dimensional 1D array where N is the number of examples\n",
    "        num_classes: Number of distinct labels that we have (10 for CIFAR-10)\n",
    "\n",
    "    returns:\n",
    "        oneHot : N X num_classes 2D array\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    n = labels.size\n",
    "    k = num_classes\n",
    "\n",
    "    matrix = np.zeros((n, k))\n",
    "    \n",
    "    # for each row, change the value specified at index y to 1\n",
    "    matrix[np.arange(n), labels] = 1\n",
    "\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainValSplit(x_train, y_train):\n",
    "\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    Creates the train-validation split (80-20 split for train-val). Please shuffle the data before creating the train-val split.\n",
    "    \"\"\"\n",
    "\n",
    "    # x_train is N X d\n",
    "    # y_train is N X 1\n",
    "\n",
    "    N = x_train.shape[0]\n",
    "    \n",
    "    # combine then shuffle\n",
    "    combined = np.column_stack((x_train, y_train))\n",
    "    np.random.shuffle(combined) # shuffles in place\n",
    "\n",
    "    \n",
    "    train_prop = np.floor(N*0.8).astype(int)\n",
    "\n",
    "    x_train_sh = combined[:train_prop, :-1]\n",
    "    y_train_sh = combined[:train_prop, -1]\n",
    "\n",
    "    x_valid_sh = combined[train_prop:, :-1]\n",
    "    y_valid_sh = combined[train_prop:, -1]\n",
    "\n",
    "    return x_train_sh, y_train_sh, x_valid_sh, y_valid_sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"\n",
    "    Loads, splits our dataset- CIFAR-10 into train, val and test sets and normalizes them\n",
    "\n",
    "    args:\n",
    "        path: Path to cifar-10 dataset\n",
    "    returns:\n",
    "        train_normalized_images, train_one_hot_labels, val_normalized_images, val_one_hot_labels,  test_normalized_images, test_one_hot_labels\n",
    "\n",
    "    \"\"\"\n",
    "    def unpickle(file):\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    cifar_path = os.path.join(path, constants.cifar10_directory)\n",
    "\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    val_images = []\n",
    "    val_labels = []\n",
    "    for i in range(1,constants.cifar10_trainBatchFiles+1):\n",
    "        images_dict = unpickle(os.path.join(cifar_path, f\"data_batch_{i}\"))\n",
    "        data = images_dict[b'data']\n",
    "        label = images_dict[b'labels']\n",
    "        train_labels.extend(label)\n",
    "        train_images.extend(data)\n",
    "    train_images = np.array(train_images)\n",
    "    train_labels = np.array(train_labels).reshape((len(train_labels),-1))\n",
    "    train_images, train_labels, val_images, val_labels = createTrainValSplit(train_images,train_labels)\n",
    "    \n",
    "    print('size of train', train_images.shape)\n",
    "\n",
    "    train_normalized_images = normalize_data(train_images)\n",
    "    train_one_hot_labels = one_hot_encoding(train_labels)\n",
    "\n",
    "    val_normalized_images = normalize_data(val_images)\n",
    "    val_one_hot_labels = one_hot_encoding(val_labels)\n",
    "\n",
    "    test_images_dict = unpickle(os.path.join(cifar_path, f\"test_batch\"))\n",
    "    test_data = test_images_dict[b'data']\n",
    "    test_labels = test_images_dict[b'labels']\n",
    "    test_images = np.array(test_data)\n",
    "    test_labels = np.array(test_labels).reshape((len(test_labels),-1))\n",
    "    test_normalized_images = normalize_data(test_images)\n",
    "    test_one_hot_labels = one_hot_encoding(test_labels)\n",
    "    \n",
    "    return train_normalized_images, train_one_hot_labels, val_normalized_images, val_one_hot_labels,  test_normalized_images, test_one_hot_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train (40000, 3072)\n",
      "now normalizing (40000, 3072)\n",
      "reshaping into (40000, 1024, 3)\n",
      "per channel (40000, 3)\n",
      "mean and std shape (40000, 3072)\n",
      "normalized shape (40000, 3072)\n",
      "now normalizing (10000, 3072)\n",
      "reshaping into (10000, 1024, 3)\n",
      "per channel (10000, 3)\n",
      "mean and std shape (10000, 3072)\n",
      "normalized shape (10000, 3072)\n",
      "now normalizing (10000, 3072)\n",
      "reshaping into (10000, 1024, 3)\n",
      "per channel (10000, 3)\n",
      "mean and std shape (10000, 3072)\n",
      "normalized shape (10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "t_x, t_y, v_x, v_y, t_x, t_y = load_data('./data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 3072)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 1024\n"
     ]
    }
   ],
   "source": [
    "out = normalize_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 3072)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_encoding(np.arange(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  7,  8],\n",
       "       [ 3,  4,  5],\n",
       "       [ 9, 10, 11],\n",
       "       [ 0,  1,  2]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.floor(N*0.8).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
